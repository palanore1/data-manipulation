{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palanore1/data-manipulation/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "IhUFKwJOxUPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "UDBoulPbHO4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sktime"
      ],
      "metadata": {
        "id": "bgz8ebaA1X4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sktime\n",
        "from sktime.datasets import load_from_tsfile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score ,confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "jr7dB5D6IfYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Racket Sports Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "VQDTJS_CxJn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = load_from_tsfile(\"/content/drive/MyDrive/ML_datasets/RacketSports_TRAIN.ts\")\n",
        "X_df_train = pd.DataFrame(train_x)\n",
        "y_df_train = pd.DataFrame(train_y, columns=['type'])\n",
        "df_racket_train = pd.concat([X_df_train, y_df_train], axis = 1)\n",
        "df_racket_train.columns=['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z', 'type']\n",
        "\n",
        "test_x, test_y = load_from_tsfile(\"/content/drive/MyDrive/ML_datasets/RacketSports_TEST.ts\")\n",
        "X_df_test = pd.DataFrame(test_x)\n",
        "y_df_test = pd.DataFrame(test_y, columns=['type'])\n",
        "df_racket_test = pd.concat([X_df_test, y_df_test], axis = 1)\n",
        "df_racket_test.columns=['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z', 'type']\n",
        "\n",
        "df_racket = pd.concat([df_racket_train, df_racket_test], ignore_index=True)"
      ],
      "metadata": {
        "id": "eqnf67AD0XOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing MITBIH and PTB Datasets"
      ],
      "metadata": {
        "id": "rmilZ9IFxbss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### MIT Dataset ##############################\n",
        "#TRAIN\n",
        "df_mit_train = pd.read_csv('/content/drive/MyDrive/ML_datasets/mitbih_train.csv', header=None)\n",
        "num_columns = len(df_mit_train.columns)\n",
        "col_names = [f\"beat_{i+1}\" for i in range(num_columns)]\n",
        "df_mit_train.columns = col_names\n",
        "df_mit_train.rename(columns={'beat_188': 'type'}, inplace=True)\n",
        "\n",
        "#TEST\n",
        "df_mit_test = pd.read_csv('/content/drive/MyDrive/ML_datasets/mitbih_test.csv', header=None)\n",
        "col_names = [f\"beat_{i+1}\" for i in range(num_columns)]\n",
        "df_mit_test.columns = col_names\n",
        "df_mit_test.rename(columns={'beat_188': 'type'}, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "################### PBT Dataset ###############################\n",
        "#NORMAL\n",
        "df_normal = pd.read_csv('/content/drive/MyDrive/ML_datasets/ptbdb_normal.csv', header=None)\n",
        "num_columns = len(df_normal.columns)\n",
        "col_names = [f\"beat_{i+1}\" for i in range(num_columns)]\n",
        "df_normal.columns = col_names\n",
        "df_normal.rename(columns={'beat_188': 'type'}, inplace=True)\n",
        "\n",
        "#ABNORMAL\n",
        "df_abnormal = pd.read_csv('/content/drive/MyDrive/ML_datasets/ptbdb_abnormal.csv', header=None)\n",
        "col_names = [f\"beat_{i+1}\" for i in range(num_columns)]\n",
        "df_abnormal.columns = col_names\n",
        "df_abnormal.rename(columns={'beat_188': 'type'}, inplace=True)\n",
        "\n",
        "#NORMAL ++ ABNORMAL\n",
        "df_ptb = pd.concat([df_normal, df_abnormal], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "svAd8h25wExL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset processing"
      ],
      "metadata": {
        "id": "H078WOrhxoJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_ptb = df_ptb.drop(columns=['type'])\n",
        "y_ptb = df_ptb['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ptb, y_ptb, test_size=0.2, random_state=42)\n",
        "\n",
        "df_PBT_train = pd.concat([X_train, y_train], axis = 1).reset_index(drop=True)\n",
        "df_PBT_test = pd.concat([X_test, y_test], axis = 1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ZzNxSgBgB-dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "81WWr5AVF02h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Class balance analysis"
      ],
      "metadata": {
        "id": "Ka9rnkmLvrg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Racket Training set"
      ],
      "metadata": {
        "id": "dy-GifOKNcvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "ax=sns.countplot(x=df_racket_train[\"type\"], width=0.5)\n",
        "\n",
        "ax.set_xticklabels(ax.get_xticklabels(), fontsize=7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UoUgfmpLNccT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting MIT train set"
      ],
      "metadata": {
        "id": "V3o-cEWPY9ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(data=df_mit_train, x = 'type')"
      ],
      "metadata": {
        "id": "-N5lN4y4ZBB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting PTB train set"
      ],
      "metadata": {
        "id": "aVi4ykdUan2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(data=df_ptb, x = 'type')"
      ],
      "metadata": {
        "id": "7a5N0GdtawOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Visualizing time series"
      ],
      "metadata": {
        "id": "zOWql-CyVrMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) RacketSports\n"
      ],
      "metadata": {
        "id": "cQ4VfJ4EVxhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* a) Afișați valorile de accelerometru pe dimensiunile x, y și z pe același\n",
        "grafic.\n",
        "* b) Afișați valorile de giroscop pe dimensiunile x, y și z pe același grafic."
      ],
      "metadata": {
        "id": "bHtoHjIRWHb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############ plotting function ########################\n",
        "\n",
        "def plot_graph(hit_type):\n",
        "    random_row = df_racket_train.loc[df_racket_train['type'] == hit_type].sample()\n",
        "    row_index = random_row.index[0]\n",
        "\n",
        "    x_acc = df_racket_train.loc[row_index, 'acc_x']\n",
        "    y_acc = df_racket_train.loc[row_index, 'acc_y']\n",
        "    z_acc = df_racket_train.loc[row_index, 'acc_z']\n",
        "    movement_type = df_racket_train.loc[row_index, 'type']\n",
        "\n",
        "    x_gyr = df_racket_train.loc[row_index, 'gyr_x']\n",
        "    y_gyr = df_racket_train.loc[row_index, 'gyr_y']\n",
        "    z_gyr = df_racket_train.loc[row_index, 'gyr_z']\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize =(16, 4), sharey=True)\n",
        "\n",
        "    axs[0].plot(x_acc, label = 'x_acc')\n",
        "    axs[0].plot(y_acc, label = 'x_acc')\n",
        "    axs[0].plot(z_acc, label = 'x_acc')\n",
        "    axs[0].set_title(f'Accelerometru {movement_type}')\n",
        "\n",
        "    axs[1].plot(x_gyr, label = 'x_gyr')\n",
        "    axs[1].plot(y_gyr, label = 'x_gyr')\n",
        "    axs[1].plot(z_gyr, label = 'x_gyr')\n",
        "    axs[1].set_title(f'Gyroscope {movement_type}')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0GsiICRLE1Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph('badminton_smash')\n",
        "print()\n",
        "plot_graph('badminton_clear')\n",
        "print()\n",
        "plot_graph('squash_forehandboast')\n",
        "print()\n",
        "plot_graph('squash_backhandboast')\n",
        "print()"
      ],
      "metadata": {
        "id": "mMYyfjeVCRQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) MIT-BIH / PTB."
      ],
      "metadata": {
        "id": "9edDmL25KrLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mit(heartbeat_type):\n",
        "    selected_row = df_mit_train.loc[df_mit_train['type'] == heartbeat_type].sample()\n",
        "    row_index = selected_row.index[0]\n",
        "\n",
        "    row_series = pd.Series(df_mit_train.iloc[row_index, :-1])\n",
        "    row_series_filtered = row_series[row_series != 0.0]\n",
        "\n",
        "    plt.plot(row_series_filtered)\n",
        "    plt.xticks([])\n",
        "    plt.title(f'heartbeat_type = {heartbeat_type}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d0L1zC7RK2-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ptb(heartbeat_type):\n",
        "    selected_row = df_PBT_train.loc[df_PBT_train['type'] == heartbeat_type].sample()\n",
        "    row_index = selected_row.index[0]\n",
        "\n",
        "    row_series = pd.Series(df_PBT_train.iloc[row_index, :-1])\n",
        "    row_series_filtered = row_series[row_series != 0.0]\n",
        "\n",
        "    plt.plot(row_series_filtered)\n",
        "    plt.xticks([])\n",
        "    if heartbeat_type == 0:\n",
        "        plt.title(f'heartbeat_type = normal')\n",
        "    else:\n",
        "        plt.title(f'heartbeat_type = abnormal')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PMv7BJA3agnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting MIT for each type of heartbeat"
      ],
      "metadata": {
        "id": "VZaBy6k5aQm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mit(0)  # plotting for MIT type 0\n",
        "print()\n",
        "plot_mit(1)  # plotting for MIT type 1\n",
        "print()\n",
        "plot_mit(2)  # plotting for MIT type 2\n",
        "print()\n",
        "plot_mit(3)  # plotting for MIT type 3\n",
        "print()\n",
        "plot_mit(4)  # plotting for MIT type 4"
      ],
      "metadata": {
        "id": "zEE9IISmX2Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting PTB for both types of heartbeats (normal/abnormal)"
      ],
      "metadata": {
        "id": "DR1YWFBNaWmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ptb(0) # plotting for PTB type normal\n",
        "print()\n",
        "plot_ptb(1) # plotting for PTB type abnormal"
      ],
      "metadata": {
        "id": "5LftjfI6aWN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Mean and standard derivation for MIT-BIH and PTB"
      ],
      "metadata": {
        "id": "Ls2k6lurbwne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mit_full = pd.concat([df_mit_train, df_mit_test]).reset_index(drop=True)\n",
        "\n",
        "df_mit_type0 = df_mit_full[df_mit_full['type']==0].reset_index(drop=True)\n",
        "df_mit_type0 = df_mit_type0.drop(columns=df_mit_type0.columns[-1])\n",
        "\n",
        "df_mit_type1 = df_mit_full[df_mit_full['type']==1].reset_index(drop=True)\n",
        "df_mit_type1 = df_mit_type1.drop(columns=df_mit_type1.columns[-1])\n",
        "\n",
        "df_mit_type2 = df_mit_full[df_mit_full['type']==2].reset_index(drop=True)\n",
        "df_mit_type2 = df_mit_type2.drop(columns=df_mit_type2.columns[-1])\n",
        "\n",
        "df_mit_type3 = df_mit_full[df_mit_full['type']==3].reset_index(drop=True)\n",
        "df_mit_type3 = df_mit_type3.drop(columns=df_mit_type3.columns[-1])\n",
        "\n",
        "df_mit_type4 = df_mit_full[df_mit_full['type']==4].reset_index(drop=True)\n",
        "df_mit_type4 = df_mit_type4.drop(columns=df_mit_type4.columns[-1])\n",
        "\n",
        "###################### type_0 ###########################\n",
        "df_mit_type0_refined = df_mit_type0.replace(0.0, np.nan)\n",
        "means_mit_type0 = df_mit_type0_refined.mean()\n",
        "stds_mit_type0 = df_mit_type0_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_mit_type0.index, means_mit_type0, yerr=stds_mit_type0, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('MIT_type0')\n",
        "plt.show()\n",
        "print()\n",
        "###################### type_1 ###########################\n",
        "df_mit_type1_refined = df_mit_type1.replace(0.0, np.nan)\n",
        "means_mit_type1 = df_mit_type1_refined.mean()\n",
        "stds_mit_type1 = df_mit_type1_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_mit_type1.index, means_mit_type1, yerr=stds_mit_type1, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('MIT_type1')\n",
        "plt.show()\n",
        "print()\n",
        "###################### type_2 ###########################\n",
        "df_mit_type2_refined = df_mit_type2.replace(0.0, np.nan)\n",
        "means_mit_type2 = df_mit_type2_refined.mean()\n",
        "stds_mit_type2 = df_mit_type2_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_mit_type2.index, means_mit_type2, yerr=stds_mit_type2, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('MIT_type2')\n",
        "plt.show()\n",
        "print()\n",
        "###################### type_3 ###########################\n",
        "df_mit_type3_refined = df_mit_type3.replace(0.0, np.nan)\n",
        "means_mit_type3 = df_mit_type3_refined.mean()\n",
        "stds_mit_type3 = df_mit_type3_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_mit_type3.index, means_mit_type3, yerr=stds_mit_type3, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('MIT_type3')\n",
        "plt.show()\n",
        "print()\n",
        "###################### type_4 ###########################\n",
        "df_mit_type4_refined = df_mit_type4.replace(0.0, np.nan)\n",
        "means_mit_type4 = df_mit_type4_refined.mean()\n",
        "stds_mit_type4 = df_mit_type4_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_mit_type4.index, means_mit_type4, yerr=stds_mit_type4, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('MIT_type4')\n",
        "plt.show()\n",
        "print()\n",
        "####################### PTB_type0 ############################\n",
        "\n",
        "df_ptb_type0 = df_ptb[df_ptb['type']==0].reset_index(drop=True)\n",
        "df_ptb_type0 = df_ptb_type0.drop(columns=df_ptb_type0.columns[-1])\n",
        "\n",
        "df_ptb_type0_refined = df_ptb_type0.replace(0.0, np.nan)\n",
        "means_ptb_type0 = df_ptb_type0_refined.mean()\n",
        "stds_ptb_type0 = df_ptb_type0_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_ptb_type0.index, means_ptb_type0, yerr=stds_ptb_type0, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('PTB_type0')\n",
        "plt.show()\n",
        "print()\n",
        "####################### PTB_type1 ############################\n",
        "\n",
        "df_ptb_type1= df_ptb[df_ptb['type']==1].reset_index(drop=True)\n",
        "df_ptb_type1 = df_ptb_type1.drop(columns=df_ptb_type1.columns[-1])\n",
        "\n",
        "df_ptb_type1_refined = df_ptb_type1.replace(0.0, np.nan)\n",
        "means_ptb_type1 = df_ptb_type1_refined.mean()\n",
        "stds_ptb_type1 = df_ptb_type1_refined.std()\n",
        "\n",
        "plt.figure(figsize=(24,6))\n",
        "plt.errorbar(means_ptb_type1.index, means_ptb_type1, yerr=stds_ptb_type1, fmt='o', capsize=3, capthick=2)\n",
        "plt.xticks([])\n",
        "plt.title('PTB_type1')\n",
        "plt.show()\n",
        "print()"
      ],
      "metadata": {
        "id": "YbcvO5ok8ET8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Data distribution per each axis for RacketSports\n"
      ],
      "metadata": {
        "id": "vDSceZm8hTas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_racket_exploded = df_racket.explode(['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z'])\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"acc_x\").add_legend()\n",
        "print()\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"acc_y\").add_legend()\n",
        "print()\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"acc_z\").add_legend()\n",
        "print()\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"gyr_x\").add_legend()\n",
        "print()\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"gyr_y\").add_legend()\n",
        "print()\n",
        "sns.FacetGrid(df_racket_exploded, hue = 'type', height=7).map(sns.distplot, \"gyr_z\").add_legend()"
      ],
      "metadata": {
        "id": "4c9xsq8ilfje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manually extracting the aributes and using classic ML algorithms\n"
      ],
      "metadata": {
        "id": "oCOq0xkRkjEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction + Standardization\n"
      ],
      "metadata": {
        "id": "T3mzMBoklAAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auxiliary functions"
      ],
      "metadata": {
        "id": "p5-quvfFfi7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def std_racket(df):\n",
        "\n",
        "  num_cols_racket = ['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z']\n",
        "  racket_num_df = df[num_cols_racket]\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  df_racket_scaled = racket_num_df.applymap(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
        "\n",
        "  df_racket_scaled=df_racket_scaled.applymap(pd.Series)\n",
        "\n",
        "  y = df.iloc[:, -1]\n",
        "\n",
        "  df_racket_std = pd.concat([df_racket_scaled, y], axis = 1)\n",
        "\n",
        "  return df_racket_std"
      ],
      "metadata": {
        "id": "9xoIPP9hjWB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def std_heartbeat(df):\n",
        "\n",
        "  X = df.iloc[:, :-1].values\n",
        "  y = df.iloc[:, -1].values\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(X)\n",
        "\n",
        "  data = np.concatenate((X, y.reshape(-1, 1)), axis = 1)\n",
        "\n",
        "  columns = [f'beat_{i+1}' for i in range(X.shape[1])] + ['type']\n",
        "  df_MIT_scaled = pd.DataFrame(data = data, columns = columns)\n",
        "\n",
        "  return df_MIT_scaled"
      ],
      "metadata": {
        "id": "pr3dfMYzlkmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_racket(df):\n",
        "    racket_features = pd.DataFrame()\n",
        "    sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z']\n",
        "    stat_functions = {\n",
        "        'mean': np.mean,\n",
        "        'std': np.std,\n",
        "        'aad': lambda x: np.mean(np.absolute(x - np.mean(x))),\n",
        "        'min': np.min,\n",
        "        'max': np.max,\n",
        "        'maxmin_diff': lambda x: np.max(x) - np.min(x),\n",
        "        'median': np.median,\n",
        "        'mad': lambda x: np.median(np.absolute(x - np.median(x))),\n",
        "        'IQR': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
        "        'neg_count': lambda x: np.sum(x < 0),\n",
        "        'pos_count': lambda x: np.sum(x > 0),\n",
        "        'above_mean_count': lambda x: np.sum(x > np.mean(x))\n",
        "    }\n",
        "\n",
        "    for col in sensor_cols:\n",
        "        for stat in stat_functions:\n",
        "            func = stat_functions[stat]\n",
        "            col_name = f'{col}_{stat}'\n",
        "            racket_features[col_name] = df[col].apply(func)\n",
        "\n",
        "    racket_features['avg_result_accl'] = [i.mean() for i in ((pd.Series(df['acc_x'])**2 + pd.Series(df['acc_y'])**2 + pd.Series(df['acc_z'])**2 + pd.Series(df['gyr_x'])**2 + pd.Series(df['gyr_y'])**2 + pd.Series(df['gyr_z'])**2)**0.5)]\n",
        "\n",
        "    racket_features['sma'] = pd.Series(df['acc_x']).apply(lambda x: np.sum(abs(x)/100)) + pd.Series(df['acc_y']).apply(lambda x: np.sum(abs(x)/100)) \\\n",
        "                  + pd.Series(df['acc_z']).apply(lambda x: np.sum(abs(x)/100)) + pd.Series(df['gyr_x']).apply(lambda x: np.sum(abs(x)/100)) \\\n",
        "                  + pd.Series(df['gyr_y']).apply(lambda x: np.sum(abs(x)/100)) + pd.Series(df['gyr_z']).apply(lambda x: np.sum(abs(x)/100))\n",
        "\n",
        "    y = df.iloc[:, -1]\n",
        "\n",
        "    racket_features = pd.concat([racket_features, y], axis = 1)\n",
        "\n",
        "    return racket_features"
      ],
      "metadata": {
        "id": "i-frWVXmIU2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_df(df):\n",
        "  df.iloc[:, :-1] = df.iloc[:, :-1].replace(0.0, np.nan)\n",
        "\n",
        "  # Window the data into segments of length 2 seconds, and include the target variable in each segment\n",
        "  segment_length = 2 * 125  # 2 seconds at 125 Hz\n",
        "  segments2 = []\n",
        "  for i in range(0, df.shape[0], segment_length):\n",
        "      segment = df.iloc[i:i+segment_length, :].values\n",
        "      if not np.isnan(segment).any():\n",
        "          target = segment[0, -1]  # Extract the target variable for this segment\n",
        "          segment_data = segment[:, :-1].flatten()\n",
        "          segments2.append((segment_data, target))\n",
        "\n",
        "  return segments2"
      ],
      "metadata": {
        "id": "aCzYypt4M2A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_heartbeat(df):\n",
        "    segments_MIT = window_df(df)\n",
        "    features = []\n",
        "    for segment_data, target in segments_MIT:\n",
        "        segment_features = {}\n",
        "        for i in range(df.shape[1]-1):\n",
        "            col_name = f'beat_{i+1}'\n",
        "            col_data = segment_data[i::df.shape[1]-1]  # Extract data from this column\n",
        "            mean = np.mean(col_data)\n",
        "            std = np.std(col_data)\n",
        "            avg_abs_dev = np.mean(np.abs(col_data - mean))\n",
        "            minimum = np.min(col_data)\n",
        "            maximum = np.max(col_data)\n",
        "            max_min_diff = maximum - minimum\n",
        "            median = np.median(col_data)\n",
        "            med_abs_dev = np.median(np.abs(col_data - median))\n",
        "            iqr = np.percentile(col_data, 75) - np.percentile(col_data, 25)\n",
        "            neg_count = np.sum(col_data < 0)\n",
        "            pos_count = np.sum(col_data > 0)\n",
        "            above_mean_count = np.sum(col_data > mean)\n",
        "            segment_features[col_name + '_mean'] = mean\n",
        "            segment_features[col_name + '_std'] = std\n",
        "            segment_features[col_name + '_avg_abs_dev'] = avg_abs_dev\n",
        "            segment_features[col_name + '_minimum'] = minimum\n",
        "            segment_features[col_name + '_maximum'] = maximum\n",
        "            segment_features[col_name + '_max_min_diff'] = max_min_diff\n",
        "            segment_features[col_name + '_median'] = median\n",
        "            segment_features[col_name + '_med_abs_dev'] = med_abs_dev\n",
        "            segment_features[col_name + '_iqr'] = iqr\n",
        "            segment_features[col_name + '_neg_count'] = neg_count\n",
        "            segment_features[col_name + '_pos_count'] = pos_count\n",
        "            segment_features[col_name + '_above_mean_count'] = above_mean_count\n",
        "        segment_features['type'] = target\n",
        "        features.append(segment_features)\n",
        "\n",
        "    # Convert the extracted features to a pandas dataframe\n",
        "    df_features = pd.DataFrame(features)\n",
        "    return df_features"
      ],
      "metadata": {
        "id": "ELRmrNBUSivh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Racket Sports"
      ],
      "metadata": {
        "id": "bFnBI-5_fJ2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizarea setului de date RacketSports"
      ],
      "metadata": {
        "id": "BczNMe5nmaeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_racket_train_std = std_racket(df_racket_train)\n",
        "df_racket_test_std = std_racket(df_racket_test)"
      ],
      "metadata": {
        "id": "zvBBXPouj5_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction for RacketSports"
      ],
      "metadata": {
        "id": "u9UJyO_CvEgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_racket_features_train = extract_features_racket(df_racket_train_std)"
      ],
      "metadata": {
        "id": "lecke2oxIaS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_racket_features_test = extract_features_racket(df_racket_test_std)"
      ],
      "metadata": {
        "id": "AHIz8VXHkjsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIT-BIH"
      ],
      "metadata": {
        "id": "BImPI-kLfOvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizarea setului de date MIT-BIH"
      ],
      "metadata": {
        "id": "O82GXE2WEMej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_MIT_train_scaled = std_heartbeat(df_mit_train)\n",
        "df_MIT_test_scaled = std_heartbeat(df_mit_test)"
      ],
      "metadata": {
        "id": "DdcAIlcOmAMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction for MIT-BIH"
      ],
      "metadata": {
        "id": "281u4QWpRtc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_MIT_features_train = extract_features_heartbeat(df_MIT_train_scaled)"
      ],
      "metadata": {
        "id": "sYXo9Nf_S6r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_MIT_features_test = extract_features_heartbeat(df_MIT_test_scaled)"
      ],
      "metadata": {
        "id": "Jz2IzvxQmm2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PBT"
      ],
      "metadata": {
        "id": "pcua5WZBfRjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizarea setului de date PBT"
      ],
      "metadata": {
        "id": "dBo3Zp5DYOt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_PBT_train_scaled = std_heartbeat(df_PBT_train)\n",
        "df_PBT_train_scaled = df_PBT_train_scaled.drop('beat_187', axis=1)\n",
        "df_PBT_test_scaled = std_heartbeat(df_PBT_test)\n",
        "df_PBT_test_scaled = df_PBT_test_scaled.drop('beat_187', axis=1)"
      ],
      "metadata": {
        "id": "SXU9VQDP2MJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction for PBT"
      ],
      "metadata": {
        "id": "9lxqK9__Y4j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_PBT_features_train = extract_features_heartbeat(df_PBT_train_scaled)"
      ],
      "metadata": {
        "id": "6kxNBTkiblos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_PBT_features_test = extract_features_heartbeat(df_PBT_test_scaled)"
      ],
      "metadata": {
        "id": "z71FH06ynpld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection + applying algorithms"
      ],
      "metadata": {
        "id": "6rt4_tVhk3so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General functions definitions"
      ],
      "metadata": {
        "id": "9-aKhqXQheob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_var_threshold2(df_features_train, df_features_test):\n",
        "  threshold = 0.1\n",
        "  selector = VarianceThreshold(threshold)\n",
        "\n",
        "  selector.fit(df_features_train)\n",
        "\n",
        "  selected_columns = df_features_train.columns[selector.get_support()]\n",
        "\n",
        "  selected_features_MIT_train = pd.DataFrame(selector.transform(df_features_train), columns = selected_columns)\n",
        "  selected_features_MIT_test = pd.DataFrame(selector.transform(df_features_test), columns = selected_columns)\n",
        "\n",
        "  return selected_features_MIT_train, selected_features_MIT_test"
      ],
      "metadata": {
        "id": "_5XDkstH4w6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_var_threshold(df_features):\n",
        "  X_train = df_features.iloc[:, :-1]\n",
        "  y_train = df_features.iloc[:, -1]\n",
        "\n",
        "  # Apply variance threshold feature selection\n",
        "  threshold = 0.1\n",
        "  selector = VarianceThreshold(threshold)\n",
        "  selector.fit(X_train)\n",
        "\n",
        "  # Get the selected features\n",
        "  selected_features = X_train.iloc[:, selector.get_support()]\n",
        "\n",
        "  selected_features = pd.concat([selected_features, y_train], axis = 1)\n",
        "\n",
        "  return selected_features\n",
        "\n",
        "def compute_select_percentile(df_features):\n",
        "  X_train = df_features.iloc[:, :-1]\n",
        "  y_train = df_features.iloc[:, -1]\n",
        "\n",
        "  # Apply SelectPercentile feature selection\n",
        "  percentile = 50\n",
        "  selector = SelectPercentile(f_classif, percentile=percentile)\n",
        "  selector.fit(X_train, y_train)\n",
        "\n",
        "  # Get the selected features\n",
        "  selected_features = X_train.iloc[:, selector.get_support()]\n",
        "  selected_features = pd.concat([selected_features, y_train], axis = 1)\n",
        "\n",
        "  return selected_features"
      ],
      "metadata": {
        "id": "3Je-iSfnUZ2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_RFC(selected_features, cv):\n",
        "  X_features = selected_features.iloc[:, :-1]\n",
        "  y_features = selected_features.iloc[:, -1]\n",
        "\n",
        "  # Define the hyperparameters to search over\n",
        "  param_grid = {\n",
        "      'n_estimators': [100, 200, 300],\n",
        "      'max_depth': [10, 20, 30],\n",
        "      'min_samples_split': [2, 4, 6],\n",
        "      'min_samples_leaf': [1, 2, 4]\n",
        "  }\n",
        "\n",
        "  # Create a Random Forest classifier\n",
        "  rf = RandomForestClassifier()\n",
        "\n",
        "  # Create a Grid Search object\n",
        "  grid_search = GridSearchCV(rf, param_grid, cv=cv)\n",
        "\n",
        "  # Fit the Grid Search object to the training data\n",
        "  grid_search.fit(X_features, y_features)\n",
        "\n",
        "  best_params_RFC = grid_search.best_params_\n",
        "  best_score_RFC = grid_search.best_score_\n",
        "\n",
        "  # Print the best hyperparameters and score\n",
        "  print(f'Best Hyperparameters: {best_params_RFC}')\n",
        "  print(f'Best Score: {best_score_RFC}')\n",
        "\n",
        "  return best_params_RFC"
      ],
      "metadata": {
        "id": "TFjASiY6XFlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_RFC_full(selected_features, selected_features_test, best_params_RFC):\n",
        "\n",
        "    X_train_r = selected_features.iloc[:, :-1]\n",
        "    X_test_r = selected_features_test.iloc[:, :-1]\n",
        "    y_train_r = selected_features.iloc[:, -1]\n",
        "    y_test_r = selected_features_test.iloc[:, -1]\n",
        "\n",
        "    rfc = RandomForestClassifier(n_estimators=best_params_RFC['n_estimators'], max_depth=best_params_RFC['max_depth'],\n",
        "                             min_samples_split=best_params_RFC['min_samples_split'],min_samples_leaf = best_params_RFC['min_samples_leaf'],\n",
        "                             random_state=42)\n",
        "\n",
        "    rfc.fit(X_train_r, y_train_r)\n",
        "\n",
        "    y_pred_r = rfc.predict(X_test_r)\n",
        "\n",
        "    accuracy_RFC = accuracy_score(y_test_r, y_pred_r)\n",
        "\n",
        "    cm_RFC = confusion_matrix(y_test_r, y_pred_r)\n",
        "\n",
        "    cr_RFC = classification_report(y_test_r, y_pred_r, zero_division = 1)\n",
        "\n",
        "    print(\"RFC Accuracy:\", accuracy_RFC)\n",
        "    print(\"RFC Confusion Matrix:\\n\", cm_RFC)\n",
        "    print(\"RFC Classification Report:\\n\", cr_RFC)\n",
        "\n",
        "    return accuracy_RFC, cm_RFC"
      ],
      "metadata": {
        "id": "a8DlKcgfvbaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_RFC(selected_features, best_params_RFC):\n",
        "    X_features = selected_features.iloc[:, :-1]\n",
        "    y_features = selected_features.iloc[:, -1]\n",
        "\n",
        "    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_features, y_features, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    rfc = RandomForestClassifier(n_estimators=best_params_RFC['n_estimators'], max_depth=best_params_RFC['max_depth'],\n",
        "                             min_samples_split=best_params_RFC['min_samples_split'],min_samples_leaf = best_params_RFC['min_samples_leaf'],\n",
        "                             random_state=42)\n",
        "\n",
        "    rfc.fit(X_train_r, y_train_r)\n",
        "\n",
        "    y_pred_r = rfc.predict(X_test_r)\n",
        "\n",
        "    accuracy_RFC = accuracy_score(y_test_r, y_pred_r)\n",
        "\n",
        "    cm_RFC = confusion_matrix(y_test_r, y_pred_r)\n",
        "\n",
        "    cr_RFC = classification_report(y_test_r, y_pred_r, zero_division = 1)\n",
        "\n",
        "    print(\"RFC Accuracy:\", accuracy_RFC)\n",
        "    print(\"RFC Confusion Matrix:\\n\", cm_RFC)\n",
        "    print(\"RFC Classification Report:\\n\", cr_RFC)\n",
        "\n",
        "    return accuracy_RFC, cm_RFC"
      ],
      "metadata": {
        "id": "jMife5SjYfTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_SVM(selected_features, cv):\n",
        "\n",
        "  X_features = selected_features.iloc[:, :-1]\n",
        "  y_features = selected_features.iloc[:, -1]\n",
        "\n",
        "  svm = SVC()\n",
        "\n",
        "  # Define the hyperparameter space\n",
        "  param_grid = {\n",
        "      'svm__C': [0.1, 1, 10],\n",
        "      'svm__kernel': ['linear', 'rbf'],\n",
        "      'svm__gamma': ['scale', 'auto']\n",
        "  }\n",
        "\n",
        "   # Define the preprocessing steps\n",
        "  preprocessor = StandardScaler()\n",
        "\n",
        "  # Define the pipeline\n",
        "  pipe = Pipeline([\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('svm', svm)\n",
        "  ])\n",
        "\n",
        "  # Define the Grid Search object\n",
        "  grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='accuracy')\n",
        "\n",
        "  # Fit the Grid Search object to the data\n",
        "  grid.fit(X_features, y_features)\n",
        "\n",
        "  best_params_SVM = grid.best_params_\n",
        "  best_score_SVM = grid.best_score_\n",
        "\n",
        "  # Print the best parameters and score\n",
        "  print(\"Best parameters:\", best_params_SVM)\n",
        "  print(\"Best score:\", best_score_SVM)\n",
        "\n",
        "  return best_params_SVM"
      ],
      "metadata": {
        "id": "AcRzwmk-cqAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_SVM2(selected_features_train, selected_features_test, best_params_SVM):\n",
        "\n",
        "  X_train_s = selected_features_train.iloc[:, :-1]\n",
        "  X_test_s = selected_features_test.iloc[:, :-1]\n",
        "  y_train_s = selected_features_train.iloc[:, -1]\n",
        "  y_test_s = selected_features_test.iloc[:, -1]\n",
        "\n",
        "  # Standardize the data\n",
        "  scaler = StandardScaler()\n",
        "  X_train_s = scaler.fit_transform(X_train_s)\n",
        "  X_test_s = scaler.transform(X_test_s)\n",
        "\n",
        "  # Train a SVM classifier on the training data\n",
        "  clf = SVC(kernel=best_params_SVM['svm__kernel'], C = best_params_SVM['svm__C'], gamma = best_params_SVM['svm__gamma'])\n",
        "  clf.fit(X_train_s, y_train_s)\n",
        "\n",
        "  # Evaluate the performance of the model on the testing data\n",
        "  y_pred_s = clf.predict(X_test_s)\n",
        "  accuracy_SVM = accuracy_score(y_test_s, y_pred_s)\n",
        "\n",
        "  cm_SVM = confusion_matrix(y_test_s, y_pred_s)\n",
        "\n",
        "  cr_SVM = classification_report(y_test_s, y_pred_s, zero_division = 1)\n",
        "\n",
        "  print(\"SVM Accuracy:\", accuracy_SVM)\n",
        "  print(\"SVM Confusion Matrix:\\n\", cm_SVM)\n",
        "  print(\"SVM Classification Report:\\n\", cr_SVM)\n",
        "\n",
        "  return accuracy_SVM, cm_SVM"
      ],
      "metadata": {
        "id": "VUUFzzHLBCRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_SVM(selected_features, best_params_SVM):\n",
        "\n",
        "  X = selected_features.drop('type', axis=1)\n",
        "  y = selected_features['type']\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Standardize the data\n",
        "  scaler = StandardScaler()\n",
        "  X_train_s = scaler.fit_transform(X_train_s)\n",
        "  X_test_s = scaler.transform(X_test_s)\n",
        "\n",
        "  # Train a SVM classifier on the training data\n",
        "  clf = SVC(kernel=best_params_SVM['svm__kernel'], C = best_params_SVM['svm__C'], gamma = best_params_SVM['svm__gamma'])\n",
        "  clf.fit(X_train_s, y_train_s)\n",
        "\n",
        "  # Evaluate the performance of the model on the testing data\n",
        "  y_pred_s = clf.predict(X_test_s)\n",
        "  accuracy_SVM = accuracy_score(y_test_s, y_pred_s)\n",
        "\n",
        "  cm_SVM = confusion_matrix(y_test_s, y_pred_s)\n",
        "\n",
        "  cr_SVM = classification_report(y_test_s, y_pred_s, zero_division = 1)\n",
        "\n",
        "  print(\"SVM Accuracy:\", accuracy_SVM)\n",
        "  print(\"SVM Confusion Matrix:\\n\", cm_SVM)\n",
        "  print(\"SVM Classification Report:\\n\", cr_SVM)\n",
        "\n",
        "  return accuracy_SVM, cm_SVM"
      ],
      "metadata": {
        "id": "44NDCxK9d45r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_GBC(selected_features, cv):\n",
        "\n",
        "  X_features = selected_features.iloc[:, :-1]\n",
        "  y_features = selected_features.iloc[:, -1]\n",
        "\n",
        "  encoder = LabelEncoder()\n",
        "  y_encoded = encoder.fit_transform(y_features)\n",
        "\n",
        "  xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "  # Define the hyperparameter space to search over\n",
        "  param_grid = {\n",
        "      'n_estimators': [50, 100, 200],\n",
        "      'learning_rate': [0.01, 0.1, 1],\n",
        "      'max_depth': [2, 3, 4],\n",
        "      'min_child_weight': [1, 5, 10]\n",
        "  }\n",
        "\n",
        "  # Define the Grid Search object\n",
        "  grid = GridSearchCV(xgb_clf, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
        "\n",
        "  # Fit the Grid Search object to the data\n",
        "  grid.fit(X_features, y_encoded)\n",
        "\n",
        "  best_params_GBC = grid.best_params_\n",
        "  best_score_GBC = grid.best_score_\n",
        "\n",
        "  # Print the best parameters and score\n",
        "  print(\"Best parameters:\", best_params_GBC)\n",
        "  print(\"Best score:\", best_score_GBC)\n",
        "\n",
        "  return best_params_GBC"
      ],
      "metadata": {
        "id": "7oEQkTlSe_g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_GBC2(selected_features_train, selected_features_test, best_params_GBC):\n",
        "\n",
        "  X_train_g = selected_features_train.iloc[:, :-1]\n",
        "  X_test_g = selected_features_test.iloc[:, :-1]\n",
        "  y_train_g = selected_features_train.iloc[:, -1]\n",
        "  y_test_g = selected_features_test.iloc[:, -1]\n",
        "\n",
        "  model = XGBClassifier(n_estimators = best_params_GBC['n_estimators'], learning_rate = best_params_GBC['learning_rate'], \\\n",
        "                                    max_depth = best_params_GBC['max_depth'], min_child_weight = best_params_GBC['min_child_weight'])\n",
        "\n",
        "  encoder = LabelEncoder()\n",
        "  y_train_g = encoder.fit_transform(y_train_g)\n",
        "  y_test_g = encoder.fit_transform(y_test_g)\n",
        "\n",
        "  # fit the model on the training data\n",
        "  model.fit(X_train_g, y_train_g)\n",
        "\n",
        "  # make predictions on the test data\n",
        "  y_pred_g = model.predict(X_test_g)\n",
        "\n",
        "  # evaluate the model using accuracy_score\n",
        "  accuracy_GBC = accuracy_score(y_test_g, y_pred_g)\n",
        "\n",
        "  cm_GBC = confusion_matrix(y_test_g, y_pred_g)\n",
        "\n",
        "  cr_GBC = classification_report(y_test_g, y_pred_g, zero_division = 1)\n",
        "\n",
        "  print(\"GBC Accuracy:\", accuracy_GBC)\n",
        "  print(\"GBC Confusion Matrix:\\n\", cm_GBC)\n",
        "  print(\"GBC Classification Report:\\n\", cr_GBC)\n",
        "\n",
        "  return accuracy_GBC, cm_GBC"
      ],
      "metadata": {
        "id": "T_tUnzthCpEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_GBC(selected_features, best_params_GBC):\n",
        "\n",
        "\n",
        "  X = selected_features.iloc[:, :-1]\n",
        "  y = selected_features.iloc[:, -1]\n",
        "\n",
        "  X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  model = XGBClassifier(n_estimators = best_params_GBC['n_estimators'], learning_rate = best_params_GBC['learning_rate'], \\\n",
        "                                    max_depth = best_params_GBC['max_depth'], min_child_weight = best_params_GBC['min_child_weight'])\n",
        "\n",
        "  encoder = LabelEncoder()\n",
        "  y_train_g = encoder.fit_transform(y_train_g)\n",
        "  y_test_g = encoder.fit_transform(y_test_g)\n",
        "\n",
        "  # fit the model on the training data\n",
        "  model.fit(X_train_g, y_train_g)\n",
        "\n",
        "  # make predictions on the test data\n",
        "  y_pred_g = model.predict(X_test_g)\n",
        "\n",
        "  # evaluate the model using accuracy_score\n",
        "  accuracy_GBC = accuracy_score(y_test_g, y_pred_g)\n",
        "\n",
        "  cm_GBC = confusion_matrix(y_test_g, y_pred_g)\n",
        "\n",
        "  cr_GBC = classification_report(y_test_g, y_pred_g, zero_division = 1)\n",
        "\n",
        "  print(\"GBC Accuracy:\", accuracy_GBC)\n",
        "  print(\"GBC Confusion Matrix:\\n\", cm_GBC)\n",
        "  print(\"GBC Classification Report:\\n\", cr_GBC)\n",
        "\n",
        "  return accuracy_GBC, cm_GBC"
      ],
      "metadata": {
        "id": "qSUnk8ubfu-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Racket Sports"
      ],
      "metadata": {
        "id": "xLnVd1h8hTzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection using Variance Treshold"
      ],
      "metadata": {
        "id": "ORyEEU2jnWD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_racket_train = compute_var_threshold(df_racket_features_train)\n",
        "selected_features_racket_test = compute_var_threshold(df_racket_features_test)"
      ],
      "metadata": {
        "id": "7lSSw1apk61R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection using Select Percentile"
      ],
      "metadata": {
        "id": "By-wLg8lnZsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_racket2 = compute_select_percentile(df_racket_features_train)"
      ],
      "metadata": {
        "id": "6HDjr9CUWvFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "vzIjRPzopPSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_racket_RFC = grid_search_RFC(selected_features_racket_train, 5)"
      ],
      "metadata": {
        "id": "PtED1xX9Xflg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "racket_accuracy_RFC, racket_cm_RFC = apply_RFC_full(selected_features_racket_train, selected_features_racket_test, best_params_racket_RFC)"
      ],
      "metadata": {
        "id": "nf9-uSv0woMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM and Grid Search with CV"
      ],
      "metadata": {
        "id": "CjcwyeI7ym3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_racket_SVM = grid_search_SVM(selected_features_racket_train, 5)"
      ],
      "metadata": {
        "id": "DKOnkwgidqe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "racket_accuracy_SVM, racket_cm_SVM = apply_SVM2(selected_features_racket_train, selected_features_racket_test, best_params_racket_SVM)"
      ],
      "metadata": {
        "id": "iXMWF9kyBkVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GradientBoostingClassifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "fdGnLCZO0-V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_racket_GBC = grid_search_GBC(selected_features_racket_train, 5)"
      ],
      "metadata": {
        "id": "pgg6z-BLfhmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "racket_accuracy_GBC, racket_cm_GBC = apply_GBC2(selected_features_racket_train, selected_features_racket_test, best_params_racket_GBC)"
      ],
      "metadata": {
        "id": "sFQmUJoADgjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(racket_cm_GBC)"
      ],
      "metadata": {
        "id": "vXzx51_u_Icb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For MIT-BIH"
      ],
      "metadata": {
        "id": "xi5GyOdSTuZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection using Variance Threshold\n"
      ],
      "metadata": {
        "id": "p1pn7kwuUIx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_MIT_train, selected_features_MIT_test = compute_var_threshold2(df_MIT_features_train, df_MIT_features_test)"
      ],
      "metadata": {
        "id": "BmN4ymJG67vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection using Select Percentile"
      ],
      "metadata": {
        "id": "mPf6PEqKh1J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_MIT_train2 = compute_select_percentile(df_MIT_features_train)\n",
        "selected_features_MIT_test2 = compute_select_percentile(df_MIT_features_test)"
      ],
      "metadata": {
        "id": "eBgsF0yFh455"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "qFjhSoYtiKvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_MIT_RFC = grid_search_RFC(selected_features_MIT_train, 3)"
      ],
      "metadata": {
        "id": "ZdLWLH2giOhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIT_accuracy_RFC, MIT_cm_RFC = apply_RFC_full(selected_features_MIT_train, selected_features_MIT_test, best_params_MIT_RFC)"
      ],
      "metadata": {
        "id": "sYGUl5G21HIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM and Grid Search with CV"
      ],
      "metadata": {
        "id": "Kv2j41XMoeDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_MIT_SVM = grid_search_SVM(selected_features_MIT_train, 3)"
      ],
      "metadata": {
        "id": "y7Y5M_oaoe0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIT_accuracy_SVM, MIT_cm_SVM = apply_SVM2(selected_features_MIT_train, selected_features_MIT_test, best_params_MIT_SVM)"
      ],
      "metadata": {
        "id": "E9ysx2mlCI_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(MIT_cm_SVM)"
      ],
      "metadata": {
        "id": "TA05bTHwDmeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "GradientBoostingClassifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "MK7W8tqdxWu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_MIT_GBC = grid_search_GBC(selected_features_MIT_train, 3)"
      ],
      "metadata": {
        "id": "UAmeHvssxeQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIT_accuracy_GBC, MIT_cm_GBC = apply_GBC2(selected_features_MIT_train, selected_features_MIT_train, best_params_MIT_GBC)"
      ],
      "metadata": {
        "id": "99MSQdHA1VL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pentru PBT"
      ],
      "metadata": {
        "id": "fLQ0i29a3psQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection using Variance Threashold"
      ],
      "metadata": {
        "id": "ofKLPXDX3tpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_PBT_train, selected_features_PBT_test = compute_var_threshold2(df_PBT_features_train, df_PBT_features_test)"
      ],
      "metadata": {
        "id": "7nWkQo3w32It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection using Select Percentile"
      ],
      "metadata": {
        "id": "3sQbLeO24LVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_PBT2 = compute_select_percentile(df_PBT_features_train)"
      ],
      "metadata": {
        "id": "oakWpC6Y4Peh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "qH215J5-4T_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_PBT_RFC = grid_search_RFC(selected_features_PBT_train, 5)"
      ],
      "metadata": {
        "id": "gyAy6uqN4ZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBT_accuracy_RFC, PBT_cm_RFC = apply_RFC(selected_features_PBT_train, best_params_PBT_RFC)"
      ],
      "metadata": {
        "id": "5QDmY00U4hb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBT_accuracy_RFC, PBT_cm_RFC = apply_RFC_full(selected_features_PBT_train, selected_features_PBT_test, best_params_PBT_RFC)"
      ],
      "metadata": {
        "id": "6OCs5jLW_D33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM and Grid Search with CV"
      ],
      "metadata": {
        "id": "W-3BDxge4syV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_PBT_SVM = grid_search_SVM(selected_features_PBT_train, 5)"
      ],
      "metadata": {
        "id": "VPE9ITRh4xIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBT_accuracy_SVM, PBT_cm_SVM = apply_SVM(selected_features_PBT_train, best_params_PBT_SVM)"
      ],
      "metadata": {
        "id": "7aKzhz4w44nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBT_accuracy_SVM, PBT_cm_SVM = apply_SVM2(selected_features_PBT_train, selected_features_PBT_test, best_params_PBT_SVM)"
      ],
      "metadata": {
        "id": "V6sR3wzxCdSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(PBT_cm_SVM)"
      ],
      "metadata": {
        "id": "4yKqUeIIExbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GradientBoostingClassifier and Grid Search with CV"
      ],
      "metadata": {
        "id": "Lh3qJM8A5Dtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_PBT_GBC = grid_search_GBC(selected_features_PBT_train, 5)"
      ],
      "metadata": {
        "id": "ulj83Ocd5K8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBT_accuracy_GBC, PBT_cm_GBC = apply_GBC2(selected_features_PBT_train, selected_features_PBT_train, best_params_PBT_GBC)"
      ],
      "metadata": {
        "id": "1q3WyN9v5R1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "IhUFKwJOxUPa",
        "VQDTJS_CxJn-",
        "rmilZ9IFxbss",
        "H078WOrhxoJg",
        "dy-GifOKNcvB",
        "V3o-cEWPY9ZL",
        "cQ4VfJ4EVxhc",
        "9edDmL25KrLN",
        "Ls2k6lurbwne",
        "vDSceZm8hTas",
        "T3mzMBoklAAM",
        "p5-quvfFfi7a",
        "bFnBI-5_fJ2o",
        "BImPI-kLfOvY",
        "pcua5WZBfRjo",
        "9-aKhqXQheob",
        "xLnVd1h8hTzP",
        "xi5GyOdSTuZ4",
        "fLQ0i29a3psQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}